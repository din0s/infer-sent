{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# ATCS Practical 1 - Demonstration & Analysis Notebook\n",
    "\n",
    "This notebook contains a demonstration of the 4 models that have been pretrained on the SNLI task, with 4 different encoder architectures:\n",
    "- **Baseline**: averaging word embeddings as sentence repr.\n",
    "- **LSTM**: unidirectional RNN on the word embeddings, last hidden state as sentence repr.\n",
    "- **BiLSTM**: bidirectional RNN, concatenation of the last hidden states of forward & backward layers as sentence repr.\n",
    "- **BiLSTM-max**: similar to BiLSTM but we apply max pooling to the word-level hidden states instead of taking the last hidden states\n",
    "\n",
    "After loading the corresponding models from the saved checkpoints, we can select a model and feed it two sentences; a premise and a hypothesis.\n",
    "The NLI classifier will then predict the relation between the sentences - either **entailment**, **neutral** or **contradiction**.\n",
    "\n",
    "Additionally, we summarize the performance of the models as evaluated on the validation and test sets of SNLI, but also using the [SentEval](https://github.com/facebookresearch/SentEval/) toolkit.\n",
    "Finally, we perform a short error analysis to identify the strengths and weaknesses of each model, in order to find which model is most suitable in each context."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, we have to define some variables for the following cells to be able to locate the required files.\n",
    "- `DATA_DIR` is the directory containing the *aligned* glove embeddings, which are loaded dynamically into the encoders.\n",
    "- `LOGS_DIR` is the directory containing the model checkpoints and the TensorBoard event files, in order to display the models' performance in a dataframe.\n",
    "- `MODEL_ORDER` is the row order used to display the results; it can be left as-is to match the one from the original paper."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# The path of the data directory (where the ALIGNED glove embeddings are)\n",
    "DATA_DIR = \"./data\"\n",
    "\n",
    "# The path of the tensorboard logs directory\n",
    "LOGS_DIR = \"./lisa_logs\"\n",
    "\n",
    "# The order in which the models should appear in the tables\n",
    "MODEL_ORDER = ['baseline', 'lstm', 'bilstm', 'bilstm-max']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Package Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we need to import all required libraries and packages. All of these should be installed with the provided `environment.yml` file.\n",
    "\n",
    "Additionally, we download the English model for the SpaCy tokenizer, which is used internally to prepare the batches."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from encoders import *\n",
    "from glove import GloVeEmbeddings\n",
    "from models import Classifier\n",
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "import glob, os, re, spacy, torch\n",
    "import pandas as pd\n",
    "\n",
    "if not spacy.util.is_package(\"en_core_web_sm\"):\n",
    "    print(\"Downloading SpaCy English model (small)\")\n",
    "    spacy.cli.download(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model Demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "At this point, we can load the GloVe embeddings from the previously specified data directory, and instanciate the tokenizer to be used later on."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading pre-trained GloVe embeddings from disk\n"
     ]
    }
   ],
   "source": [
    "glove = GloVeEmbeddings(DATA_DIR)\n",
    "tokenizer = get_tokenizer('spacy', language='en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We search in the specified log directory to find all saved checkpoints, which the pretrained models will be loaded from."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "CKPTS_GLOB = \"*/*/checkpoints/*.ckpt\"\n",
    "CKPTS_PATTERN = r\"([^\\/]+)\\/version_\\d+.*\\.ckpt\"\n",
    "\n",
    "EMBED_DIM = 300\n",
    "LSTM_STATE_DIM = 2048\n",
    "\n",
    "models = {}\n",
    "for ckpt_name in glob.glob(os.path.join(LOGS_DIR, CKPTS_GLOB)):\n",
    "    # Extract model name from checkpoint name\n",
    "    res = re.search(CKPTS_PATTERN, ckpt_name)\n",
    "    model_name = res.group(1)\n",
    "\n",
    "    if model_name == \"baseline\":\n",
    "        repr_dim = EMBED_DIM\n",
    "        encoder = BaselineEncoder()\n",
    "    else:\n",
    "        repr_dim = LSTM_STATE_DIM\n",
    "\n",
    "        if model_name == \"lstm\":\n",
    "            encoder = LSTMEncoder(EMBED_DIM, LSTM_STATE_DIM)\n",
    "        elif model_name == \"bilstm\":\n",
    "            repr_dim *= 2\n",
    "            encoder = BiLSTMEncoder(EMBED_DIM, LSTM_STATE_DIM)\n",
    "        elif model_name == \"bilstm-max\":\n",
    "            repr_dim *= 2\n",
    "            encoder = MaxBiLSTMEncoder(EMBED_DIM, LSTM_STATE_DIM)\n",
    "        else:\n",
    "            print(f\"Encountered unsupported encoder architecture '{model_name}'\")\n",
    "            continue\n",
    "\n",
    "    model_args = {\"embeddings\": glove.vectors, \"encoder\": encoder}\n",
    "    model = Classifier.load_from_checkpoint(ckpt_name, **model_args)\n",
    "    model.load_embeddings(glove.vectors)\n",
    "    models[model_name] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We simply define a function that takes a model name, a premise and a hypothesis as arguments, and returns the predicted relation class.\n",
    "\n",
    "Inside this function, we can see the preprocessing pipeline that was applied to the dataset before training the NLI classifier."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "INT_TO_CLASS = {\n",
    "    0: \"entailment\",\n",
    "    1: \"neutral\",\n",
    "    2: \"contradiction\"\n",
    "}\n",
    "\n",
    "@torch.no_grad()\n",
    "def inference(model_name: str, premise: str, hypothesis: str) -> str:\n",
    "    if model_name not in models:\n",
    "        raise Exception(f\"Unknown encoder type '{model_name}'!\")\n",
    "\n",
    "    # Load model from dict\n",
    "    model = models[model_name]\n",
    "\n",
    "    # Lowercase + tokenize\n",
    "    premise = tokenizer(premise.lower())\n",
    "    hypothesis = tokenizer(hypothesis.lower())\n",
    "\n",
    "    # Convert list of tokens to list of IDs\n",
    "    premise = [glove.get_id(t) for t in premise]\n",
    "    hypothesis = [glove.get_id(t) for t in hypothesis]\n",
    "\n",
    "    # Convert to tensors with an extra dimension (batch_size=1)\n",
    "    p = torch.IntTensor(premise).unsqueeze(0)\n",
    "    h = torch.IntTensor(hypothesis).unsqueeze(0)\n",
    "\n",
    "    # Count length of each sentence\n",
    "    p_len = torch.LongTensor([len(premise)])\n",
    "    h_len = torch.LongTensor([len(hypothesis)])\n",
    "\n",
    "    logits = model(p, h, p_len, h_len)\n",
    "    category = INT_TO_CLASS[logits.argmax().item()]\n",
    "    return category"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is an **interactive** cell, meaning that we can play around with the 3 values.\n",
    "- `MODEL_NAME` specifies which model to use; thus, it has to be one of the 4 supported architectures.\n",
    "- `PREMISE` corresponds to the first of the two sentences that will be fed to the model.\n",
    "- `HYPOTHESIS` corresponds to the second sentence."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'contradiction'"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# must be one of: 'baseline', 'lstm', 'bilstm', 'bilstm-max'\n",
    "MODEL_NAME = 'bilstm-max'\n",
    "\n",
    "PREMISE = 'The dog is eating.'\n",
    "HYPOTHESIS = 'The dog sleeps.'\n",
    "\n",
    "inference(MODEL_NAME, PREMISE, HYPOTHESIS)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see, our BiLSTM-max model correctly inferred that the sentences `The dog is eating.` and `The dog sleeps.` are contradicting each other.\n",
    "\n",
    "You can see how the model's prediction will change when you try out different sentences.\n",
    "\n",
    "We will perform a more extensive analysis focusing on the errors later on."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Results Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's recreate the two tables from the original paper, which showcase the performance of our models both in the original task (SNLI) and the transfer tasks using SentEval."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, we define a few functions that are used to modify the display style of the dataframes we will create later on (purely for aesthetic reasons)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def highlight_max(s):\n",
    "    is_max = s == s.max()\n",
    "    return ['font-weight: bold' if cell else '' for cell in is_max]\n",
    "\n",
    "def format_df(df):\n",
    "    dfs = df.style\n",
    "    dfs = dfs.apply(highlight_max)\n",
    "    dfs = dfs.format(\"{:2.2f}\")\n",
    "    dfs = dfs.set_table_styles([\n",
    "        dict(selector='thead th', props=[('text-align', 'center'), ('vertical-align', 'bottom')]),\n",
    "        dict(selector='td', props=[('text-align', 'center'), ('padding', '0.5em 1.5em')]),\n",
    "    ])\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Performance Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using the specified logs directory, we access the TensorBoard event files and extract the recorded validation/test accuracy for each model on the SNLI dataset."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "LOGS_GLOB = \"*/*\"\n",
    "LOGS_PATTERN = r\"([^\\/]+)\\/((?:version_\\d+)|(?:eval))\"\n",
    "\n",
    "nli_df = pd.DataFrame(columns=['dev', 'test'])\n",
    "for log_name in glob.glob(os.path.join(LOGS_DIR, LOGS_GLOB)):\n",
    "    # Extract model & version name from logfile name\n",
    "    res = re.search(LOGS_PATTERN, log_name)\n",
    "    model_name = res.group(1)\n",
    "    is_test = res.group(2) == \"eval\"\n",
    "\n",
    "    # Read the TFEvents file\n",
    "    ea = EventAccumulator(log_name)\n",
    "    ea.Reload()\n",
    "\n",
    "    if is_test:\n",
    "        # Read the test_acc value\n",
    "        acc = ea.Scalars('test_acc')[0].value\n",
    "    else:\n",
    "        # Read all val_acc values and pick the maximum\n",
    "        acc = max(map(lambda e: e.value, ea.Scalars('val_acc')))\n",
    "\n",
    "    # Convert accuracy to percentage\n",
    "    acc *= 100\n",
    "\n",
    "    col_name = 'test' if is_test else 'dev'\n",
    "    if model_name not in nli_df.index:\n",
    "        acc_df = pd.DataFrame.from_dict({col_name: [acc]})\n",
    "        acc_df.index = [model_name]\n",
    "\n",
    "        nli_df = pd.concat((nli_df, acc_df))\n",
    "    else:\n",
    "        nli_df.at[model_name, col_name] = acc"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We create a helper function that calculates the micro and macro accuracy of the validation sets used in the SentEval tasks.\n",
    "\n",
    "The input of this function is a dataframe which represents the entire SentEval result dict for a given model, along with the model's name to use in the resulting dataframe."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(df: pd.DataFrame, name: str) -> pd.DataFrame:\n",
    "    # Filter out columns that don't have a validation accuracy\n",
    "    # This is the case in non-classification tasks, such as SICK-R and STS14\n",
    "    df = df.loc[:, df.loc['devacc'].notnull()]\n",
    "\n",
    "    # Extract the validation accuracy for each task\n",
    "    val_acc = df.loc['devacc']\n",
    "\n",
    "    # Calculate the weighing factor for micro-accuracy\n",
    "    n_val = df.loc['ndev']\n",
    "    weight = n_val / n_val.sum()\n",
    "\n",
    "    # Calculate the macro and micro accuracy\n",
    "    macro = val_acc.mean()\n",
    "    micro = (val_acc * weight).sum()\n",
    "\n",
    "    # Return metrics as dataframe\n",
    "    acc_dict = {'micro': [micro], 'macro': [macro]}\n",
    "    acc_df = pd.DataFrame.from_dict(acc_dict)\n",
    "    acc_df.index = [name]\n",
    "    return acc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using the specified logs directory once again, we access the JSON files containing the SentEval results for each model, and extract the (micro/macro) validation accuracy."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "RESULTS_GLOB = \"results_*.json\"\n",
    "RESULTS_PATTERN = r\"results*_([^\\.]+)\\.json\"\n",
    "\n",
    "transfer_df = pd.DataFrame()\n",
    "for results_file in glob.glob(os.path.join(LOGS_DIR, RESULTS_GLOB)):\n",
    "    # Extract model name from file name\n",
    "    res = re.search(RESULTS_PATTERN, results_file)\n",
    "    model_name = res.group(1)\n",
    "\n",
    "    # Convert json to dataframe\n",
    "    df = pd.read_json(results_file)\n",
    "    # Calculate accuracies and create dataframe row\n",
    "    model_accs = calculate_accuracy(df, model_name)\n",
    "\n",
    "    # Append row to transfer results dataframe\n",
    "    transfer_df = pd.concat((transfer_df, model_accs))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we concatenate the two \"subtables\" to produce Table 3 from the original paper."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<pandas.io.formats.style.Styler at 0x7fc45924f460>",
      "text/html": "<style  type=\"text/css\" >\n    #T_b968c_ thead th {\n          text-align: center;\n          vertical-align: bottom;\n    }    #T_b968c_ td {\n          text-align: center;\n          padding: 0.5em 1.5em;\n    }#T_b968c_row3_col0,#T_b968c_row3_col1,#T_b968c_row3_col2,#T_b968c_row3_col3{\n            font-weight:  bold;\n        }</style><table id=\"T_b968c_\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" colspan=\"2\">NLI</th>        <th class=\"col_heading level0 col2\" colspan=\"2\">Transfer</th>    </tr>    <tr>        <th class=\"blank level1\" ></th>        <th class=\"col_heading level1 col0\" >dev</th>        <th class=\"col_heading level1 col1\" >test</th>        <th class=\"col_heading level1 col2\" >micro</th>        <th class=\"col_heading level1 col3\" >macro</th>    </tr></thead><tbody>\n                <tr>\n                        <th id=\"T_b968c_level0_row0\" class=\"row_heading level0 row0\" >baseline</th>\n                        <td id=\"T_b968c_row0_col0\" class=\"data row0 col0\" >65.72</td>\n                        <td id=\"T_b968c_row0_col1\" class=\"data row0 col1\" >65.33</td>\n                        <td id=\"T_b968c_row0_col2\" class=\"data row0 col2\" >80.70</td>\n                        <td id=\"T_b968c_row0_col3\" class=\"data row0 col3\" >79.09</td>\n            </tr>\n            <tr>\n                        <th id=\"T_b968c_level0_row1\" class=\"row_heading level0 row1\" >lstm</th>\n                        <td id=\"T_b968c_row1_col0\" class=\"data row1 col0\" >81.45</td>\n                        <td id=\"T_b968c_row1_col1\" class=\"data row1 col1\" >81.26</td>\n                        <td id=\"T_b968c_row1_col2\" class=\"data row1 col2\" >78.19</td>\n                        <td id=\"T_b968c_row1_col3\" class=\"data row1 col3\" >77.56</td>\n            </tr>\n            <tr>\n                        <th id=\"T_b968c_level0_row2\" class=\"row_heading level0 row2\" >bilstm</th>\n                        <td id=\"T_b968c_row2_col0\" class=\"data row2 col0\" >80.87</td>\n                        <td id=\"T_b968c_row2_col1\" class=\"data row2 col1\" >80.66</td>\n                        <td id=\"T_b968c_row2_col2\" class=\"data row2 col2\" >81.07</td>\n                        <td id=\"T_b968c_row2_col3\" class=\"data row2 col3\" >80.54</td>\n            </tr>\n            <tr>\n                        <th id=\"T_b968c_level0_row3\" class=\"row_heading level0 row3\" >bilstm-max</th>\n                        <td id=\"T_b968c_row3_col0\" class=\"data row3 col0\" >84.37</td>\n                        <td id=\"T_b968c_row3_col1\" class=\"data row3 col1\" >83.85</td>\n                        <td id=\"T_b968c_row3_col2\" class=\"data row3 col2\" >82.53</td>\n                        <td id=\"T_b968c_row3_col3\" class=\"data row3 col3\" >82.06</td>\n            </tr>\n    </tbody></table>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance_df = pd.concat((nli_df, transfer_df), axis=1, keys=['NLI', 'Transfer'])\n",
    "performance_df.reindex(MODEL_ORDER)\n",
    "\n",
    "format_df(performance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### SentEval Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "By iterating over the SentEval results from the JSON files again, we now aggregate all performance information across all models to generate Table 4 from the original paper."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<pandas.io.formats.style.Styler at 0x7fc458f36370>",
      "text/html": "<style  type=\"text/css\" >\n    #T_166ca_ thead th {\n          text-align: center;\n          vertical-align: bottom;\n    }    #T_166ca_ td {\n          text-align: center;\n          padding: 0.5em 1.5em;\n    }#T_166ca_row3_col0,#T_166ca_row3_col1,#T_166ca_row3_col2,#T_166ca_row3_col3,#T_166ca_row3_col4,#T_166ca_row3_col5,#T_166ca_row3_col6,#T_166ca_row3_col7,#T_166ca_row3_col8,#T_166ca_row3_col9,#T_166ca_row3_col10,#T_166ca_row3_col11{\n            font-weight:  bold;\n        }</style><table id=\"T_166ca_\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >MR</th>        <th class=\"col_heading level0 col1\" >CR</th>        <th class=\"col_heading level0 col2\" >SUBJ</th>        <th class=\"col_heading level0 col3\" >MPQA</th>        <th class=\"col_heading level0 col4\" >SST</th>        <th class=\"col_heading level0 col5\" >TREC</th>        <th class=\"col_heading level0 col6\" >MRPC<br>accuracy</th>        <th class=\"col_heading level0 col7\" >MRPC<br>F1-score</th>        <th class=\"col_heading level0 col8\" >SICK-R</th>        <th class=\"col_heading level0 col9\" >SICK-E</th>        <th class=\"col_heading level0 col10\" >STS14<br>average<br>pearson</th>        <th class=\"col_heading level0 col11\" >STS14<br>weighted<br>pearson</th>    </tr></thead><tbody>\n                <tr>\n                        <th id=\"T_166ca_level0_row0\" class=\"row_heading level0 row0\" >baseline</th>\n                        <td id=\"T_166ca_row0_col0\" class=\"data row0 col0\" >75.11</td>\n                        <td id=\"T_166ca_row0_col1\" class=\"data row0 col1\" >79.31</td>\n                        <td id=\"T_166ca_row0_col2\" class=\"data row0 col2\" >90.57</td>\n                        <td id=\"T_166ca_row0_col3\" class=\"data row0 col3\" >84.77</td>\n                        <td id=\"T_166ca_row0_col4\" class=\"data row0 col4\" >78.25</td>\n                        <td id=\"T_166ca_row0_col5\" class=\"data row0 col5\" >81.00</td>\n                        <td id=\"T_166ca_row0_col6\" class=\"data row0 col6\" >72.52</td>\n                        <td id=\"T_166ca_row0_col7\" class=\"data row0 col7\" >81.24</td>\n                        <td id=\"T_166ca_row0_col8\" class=\"data row0 col8\" >0.80</td>\n                        <td id=\"T_166ca_row0_col9\" class=\"data row0 col9\" >78.73</td>\n                        <td id=\"T_166ca_row0_col10\" class=\"data row0 col10\" >0.45</td>\n                        <td id=\"T_166ca_row0_col11\" class=\"data row0 col11\" >0.46</td>\n            </tr>\n            <tr>\n                        <th id=\"T_166ca_level0_row1\" class=\"row_heading level0 row1\" >lstm</th>\n                        <td id=\"T_166ca_row1_col0\" class=\"data row1 col0\" >72.49</td>\n                        <td id=\"T_166ca_row1_col1\" class=\"data row1 col1\" >76.72</td>\n                        <td id=\"T_166ca_row1_col2\" class=\"data row1 col2\" >86.61</td>\n                        <td id=\"T_166ca_row1_col3\" class=\"data row1 col3\" >85.03</td>\n                        <td id=\"T_166ca_row1_col4\" class=\"data row1 col4\" >76.72</td>\n                        <td id=\"T_166ca_row1_col5\" class=\"data row1 col5\" >78.60</td>\n                        <td id=\"T_166ca_row1_col6\" class=\"data row1 col6\" >72.41</td>\n                        <td id=\"T_166ca_row1_col7\" class=\"data row1 col7\" >81.51</td>\n                        <td id=\"T_166ca_row1_col8\" class=\"data row1 col8\" >0.86</td>\n                        <td id=\"T_166ca_row1_col9\" class=\"data row1 col9\" >84.45</td>\n                        <td id=\"T_166ca_row1_col10\" class=\"data row1 col10\" >0.55</td>\n                        <td id=\"T_166ca_row1_col11\" class=\"data row1 col11\" >0.56</td>\n            </tr>\n            <tr>\n                        <th id=\"T_166ca_level0_row2\" class=\"row_heading level0 row2\" >bilstm</th>\n                        <td id=\"T_166ca_row2_col0\" class=\"data row2 col0\" >73.02</td>\n                        <td id=\"T_166ca_row2_col1\" class=\"data row2 col1\" >78.49</td>\n                        <td id=\"T_166ca_row2_col2\" class=\"data row2 col2\" >89.95</td>\n                        <td id=\"T_166ca_row2_col3\" class=\"data row2 col3\" >84.98</td>\n                        <td id=\"T_166ca_row2_col4\" class=\"data row2 col4\" >78.42</td>\n                        <td id=\"T_166ca_row2_col5\" class=\"data row2 col5\" >86.40</td>\n                        <td id=\"T_166ca_row2_col6\" class=\"data row2 col6\" >71.13</td>\n                        <td id=\"T_166ca_row2_col7\" class=\"data row2 col7\" >80.08</td>\n                        <td id=\"T_166ca_row2_col8\" class=\"data row2 col8\" >0.87</td>\n                        <td id=\"T_166ca_row2_col9\" class=\"data row2 col9\" >83.84</td>\n                        <td id=\"T_166ca_row2_col10\" class=\"data row2 col10\" >0.56</td>\n                        <td id=\"T_166ca_row2_col11\" class=\"data row2 col11\" >0.58</td>\n            </tr>\n            <tr>\n                        <th id=\"T_166ca_level0_row3\" class=\"row_heading level0 row3\" >bilstm-max</th>\n                        <td id=\"T_166ca_row3_col0\" class=\"data row3 col0\" >75.40</td>\n                        <td id=\"T_166ca_row3_col1\" class=\"data row3 col1\" >81.14</td>\n                        <td id=\"T_166ca_row3_col2\" class=\"data row3 col2\" >91.45</td>\n                        <td id=\"T_166ca_row3_col3\" class=\"data row3 col3\" >85.47</td>\n                        <td id=\"T_166ca_row3_col4\" class=\"data row3 col4\" >79.19</td>\n                        <td id=\"T_166ca_row3_col5\" class=\"data row3 col5\" >87.20</td>\n                        <td id=\"T_166ca_row3_col6\" class=\"data row3 col6\" >74.09</td>\n                        <td id=\"T_166ca_row3_col7\" class=\"data row3 col7\" >81.72</td>\n                        <td id=\"T_166ca_row3_col8\" class=\"data row3 col8\" >0.88</td>\n                        <td id=\"T_166ca_row3_col9\" class=\"data row3 col9\" >85.24</td>\n                        <td id=\"T_166ca_row3_col10\" class=\"data row3 col10\" >0.63</td>\n                        <td id=\"T_166ca_row3_col11\" class=\"data row3 col11\" >0.65</td>\n            </tr>\n    </tbody></table>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RESULTS_GLOB = \"results_*.json\"\n",
    "RESULTS_PATTERN = r\"results*_([^\\.]+)\\.json\"\n",
    "\n",
    "senteval_df = pd.DataFrame()\n",
    "for results_file in glob.glob(os.path.join(LOGS_DIR, RESULTS_GLOB)):\n",
    "    # Extract model name from file name\n",
    "    res = re.search(RESULTS_PATTERN, results_file)\n",
    "    model_name = res.group(1)\n",
    "\n",
    "    # Convert json to dataframe\n",
    "    df = pd.read_json(results_file)\n",
    "\n",
    "    # Select accuracy for classification tasks (except MRPC)\n",
    "    df_class = df.loc[['acc'], df.loc['acc'].notnull()].drop('MRPC', axis=1)\n",
    "    df_class.index = [model_name]\n",
    "\n",
    "    # Select accuracy and F1 score for the MRPC task\n",
    "    mrpc_cols = pd.MultiIndex.from_product((['MRPC'], ['acc','f1']))\n",
    "    mrpc_vals = df.loc[['acc', 'f1'], 'MRPC'].array\n",
    "    df_mrpc = pd.DataFrame([mrpc_vals], columns=mrpc_cols, index=[model_name])\n",
    "\n",
    "    # Select pearson value for the SICK-R task\n",
    "    df_sickr = pd.DataFrame(df.loc[['pearson'], 'SICKRelatedness'])\n",
    "    df_sickr.index = [model_name]\n",
    "\n",
    "    # Select pearson value(s) for the STS14 task\n",
    "    dict_sts14 = df.loc['all', 'STS14']['pearson']\n",
    "    sts14_cols = pd.MultiIndex.from_product((['STS14'], dict_sts14.keys()))\n",
    "    df_sts14 = pd.DataFrame([dict_sts14.values()], columns=sts14_cols, index=[model_name])\n",
    "\n",
    "    # Concat all tasks to one dataframe row\n",
    "    scores_df = pd.concat((df_class, df_mrpc, df_sickr, df_sts14), axis=1)\n",
    "\n",
    "    # Append row to to SentEval scores dataframe\n",
    "    senteval_df = pd.concat((senteval_df, scores_df), axis=0)\n",
    "\n",
    "TASK_ORDER = [\n",
    "    'MR', 'CR', 'SUBJ', 'MPQA', 'SST2', 'TREC',\n",
    "    ('MRPC', 'acc'), ('MRPC', 'f1'),\n",
    "    'SICKRelatedness', 'SICKEntailment',\n",
    "    ('STS14', 'mean'), ('STS14', 'wmean')\n",
    "]\n",
    "\n",
    "TASK_NAMES = TASK_ORDER.copy()\n",
    "TASK_NAMES[4] = 'SST'\n",
    "TASK_NAMES[6] = 'MRPC<br>accuracy' ; TASK_NAMES[7] = 'MRPC<br>F1-score'\n",
    "TASK_NAMES[-4] = 'SICK-R' ; TASK_NAMES[-3] = 'SICK-E'\n",
    "TASK_NAMES[-2] = 'STS14<br>average<br>pearson' ; TASK_NAMES[-1] = 'STS14<br>weighted<br>pearson'\n",
    "\n",
    "senteval_df = senteval_df.reindex(MODEL_ORDER).T.reindex(TASK_ORDER).T\n",
    "senteval_df.columns = TASK_NAMES  # rename to match the paper's order\n",
    "\n",
    "format_df(senteval_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we can take a closer look into some more concrete examples that showcase where each model outperforms the rest, and where each model fails."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# TODO"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}